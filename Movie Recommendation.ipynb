{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MwOr0VrrOv4Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nOdsctkuOv4R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/suhasmathey/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/suhasmathey/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_directory = os.getcwd()\n",
        "\n",
        "file_path = os.path.join(current_directory, \"imdb-movies-dataset.csv\")\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing the Data - Removing the stop words and lemmatizing the words in the movie descriptions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    words = text.split() #Splitting text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words] #If the word is not in stop_words, it is included in filtered_words\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "data['Description'] = data['Description'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       SolÃ¨ne, 40-year-old single mom, begin unexpect...\n",
            "1       Many year reign Caesar, young ape go journey l...\n",
            "2       1963 Michigan, business rival Kellogg's Post c...\n",
            "3       down-and-out stuntman must find missing star e...\n",
            "4       Tashi, former tennis prodigy turned coach, tur...\n",
            "                              ...                        \n",
            "9995    dramatic life trapeze artists, clown, elephant...\n",
            "9996    lone sellsword named Guts get recruited mercen...\n",
            "9997    couple twelve-year-old Norwegian girl struggle...\n",
            "9998    journalist strike romantic relationship notori...\n",
            "9999    widow widower find relationship developing lov...\n",
            "Name: Description, Length: 10000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenizing the words in the description \n",
        "\n",
        "Tokenizer - Breaking down text into smaller units, \"tokens\"\n",
        "\n",
        "texts_to_sequence - Converting text into a sequence of integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 50)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=7000)\n",
        "tokenizer.fit_on_texts(data['Description'])\n",
        "word_index = tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(data['Description'])\n",
        "X = pad_sequences(X, maxlen=50)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_glove_embeddings(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_filepath = os.path.join(current_directory, \"glove.42B.300d.txt\")\n",
        "glove_embeddings = load_glove_embeddings(glove_filepath)\n",
        "\n",
        "\n",
        "\n",
        "embedding_dim = 300\n",
        "num_words = min(7000, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encoding genre labels into a binary format\n",
        "\n",
        ".fillna('') - Fills cells with NaN values with empty strings, preventing from encoutering float objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Genre'] = data['Genre'].fillna('')\n",
        "data['Genre'] = data['Genre'].apply(lambda x: x.split(' , '))\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(data['Genre'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-17 23:20:57.594316: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
            "2024-06-17 23:20:57.594423: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
            "2024-06-17 23:20:57.594444: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
            "2024-06-17 23:20:57.594942: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-06-17 23:20:57.595515: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-17 23:20:58.978937: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "154/154 [==============================] - 7s 35ms/step - loss: 0.0664 - accuracy: 0.0343 - val_loss: 0.0128 - val_accuracy: 0.0348\n",
            "Epoch 2/20\n",
            "154/154 [==============================] - 4s 26ms/step - loss: 0.0127 - accuracy: 0.0461 - val_loss: 0.0127 - val_accuracy: 0.0557\n",
            "Epoch 3/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0127 - accuracy: 0.0445 - val_loss: 0.0127 - val_accuracy: 0.0557\n",
            "Epoch 4/20\n",
            "154/154 [==============================] - 4s 24ms/step - loss: 0.0127 - accuracy: 0.0429 - val_loss: 0.0128 - val_accuracy: 0.0348\n",
            "Epoch 5/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0127 - accuracy: 0.0469 - val_loss: 0.0128 - val_accuracy: 0.0557\n",
            "Epoch 6/20\n",
            "154/154 [==============================] - 4s 25ms/step - loss: 0.0127 - accuracy: 0.0437 - val_loss: 0.0127 - val_accuracy: 0.0348\n",
            "Epoch 7/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0127 - accuracy: 0.0447 - val_loss: 0.0128 - val_accuracy: 0.0557\n",
            "Epoch 8/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0127 - accuracy: 0.0461 - val_loss: 0.0127 - val_accuracy: 0.0557\n",
            "Epoch 9/20\n",
            "154/154 [==============================] - 3s 22ms/step - loss: 0.0127 - accuracy: 0.0478 - val_loss: 0.0128 - val_accuracy: 0.0557\n",
            "Epoch 10/20\n",
            "154/154 [==============================] - 3s 23ms/step - loss: 0.0127 - accuracy: 0.0443 - val_loss: 0.0128 - val_accuracy: 0.0538\n",
            "Epoch 11/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0127 - accuracy: 0.0492 - val_loss: 0.0128 - val_accuracy: 0.0357\n",
            "Epoch 12/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0126 - accuracy: 0.0465 - val_loss: 0.0127 - val_accuracy: 0.0595\n",
            "Epoch 13/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0126 - accuracy: 0.0490 - val_loss: 0.0126 - val_accuracy: 0.0557\n",
            "Epoch 14/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0125 - accuracy: 0.0531 - val_loss: 0.0126 - val_accuracy: 0.0619\n",
            "Epoch 15/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0124 - accuracy: 0.0618 - val_loss: 0.0125 - val_accuracy: 0.0776\n",
            "Epoch 16/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0123 - accuracy: 0.0796 - val_loss: 0.0123 - val_accuracy: 0.0457\n",
            "Epoch 17/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0120 - accuracy: 0.0835 - val_loss: 0.0122 - val_accuracy: 0.0648\n",
            "Epoch 18/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0118 - accuracy: 0.0890 - val_loss: 0.0121 - val_accuracy: 0.0729\n",
            "Epoch 19/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0116 - accuracy: 0.1012 - val_loss: 0.0120 - val_accuracy: 0.0762\n",
            "Epoch 20/20\n",
            "154/154 [==============================] - 4s 23ms/step - loss: 0.0114 - accuracy: 0.1124 - val_loss: 0.0119 - val_accuracy: 0.0829\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x3fe680890>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, weights = [embedding_matrix], input_length=50))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0119 - accuracy: 0.0740\n",
            "Test Loss: 0.011853113770484924\n",
            "Test Accuracy: 0.07400000095367432\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
